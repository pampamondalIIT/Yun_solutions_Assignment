{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense\n",
        "from keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "historical_data = pd.read_csv('data.csv')\n",
        "\n",
        "historical_data['Expiry'] = pd.to_datetime(historical_data['Expiry'])\n",
        "historical_data['Date'] = pd.to_datetime(historical_data['Date'])\n",
        "historical_data['days_to_expiry'] = (historical_data['Expiry'] - historical_data['Date']).dt.days\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIvCJvQ1uYcK",
        "outputId": "ad85d3e6-f6be-4bf6-e15f-c35886f04440"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-38-8b60227b9c2b>:14: UserWarning: Parsing dates in DD/MM/YYYY format when dayfirst=False (the default) was specified. This may lead to inconsistently parsed dates! Specify a format to ensure consistent parsing.\n",
            "  historical_data['Expiry'] = pd.to_datetime(historical_data['Expiry'])\n",
            "<ipython-input-38-8b60227b9c2b>:15: UserWarning: Parsing dates in DD/MM/YYYY format when dayfirst=False (the default) was specified. This may lead to inconsistently parsed dates! Specify a format to ensure consistent parsing.\n",
            "  historical_data['Date'] = pd.to_datetime(historical_data['Date'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature engineering\n",
        "import tensorflow as tf\n",
        "#print(historical_data.columns)\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "features_to_scale = ['index_Open', 'index_High', 'index_Low', 'index_Close', 'index_Gap']\n",
        "X_to_scale = historical_data[features_to_scale].values\n",
        "X_to_keep = historical_data['days_to_expiry'].values\n",
        "\n",
        "scaler_X = MinMaxScaler(feature_range=(0, 1))\n",
        "print(X_to_scale.shape)\n",
        "X_scaled = scaler_X.fit_transform(X_to_scale)\n",
        "\n",
        "X_final = np.hstack((X_scaled, X_to_keep.reshape(-1, 1)))\n",
        "#print(X_final)\n",
        "y = historical_data[['straddle_Open', 'straddle_High', 'straddle_Low', 'straddle_Close']].values\n",
        "\n",
        "scaler_y = MinMaxScaler(feature_range=(0, 1))\n",
        "\n",
        "y_scaled = scaler_y.fit_transform(y)\n",
        "\n",
        "# Reshape input data for LSTM\n",
        "X_final = X_final.reshape((X_final.shape[0], 1, X_final.shape[1]))\n",
        "\n",
        "# Data splitting\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_final, y_scaled, test_size=0.2, random_state=42)\n",
        "\n",
        "# Model building\n",
        "model = Sequential()\n",
        "model.add(LSTM(units=50, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "model.add(Dense(units=4))  # 4 outputs for Straddle Open, High, Low, Close\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "model.compile(optimizer= optimizer, loss='mse')\n",
        "\n",
        "# Model training\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='min', restore_best_weights=True)\n",
        "history = model.fit(X_train, y_train, epochs=100, validation_data=(X_test , y_test) , callbacks=(early_stopping))\n",
        "\n",
        "\n",
        "predictions = model.predict(X_test)\n",
        "predictions_unscaled = scaler_y.inverse_transform(predictions)\n",
        "print(predictions_unscaled)\n",
        "best_val_loss = min(history.history['val_loss'])\n",
        "print(\"Best Validation Loss:\", best_val_loss)\n",
        "best_loss = min(history.history['loss'])\n",
        "print(\"Best Training Loss:\", best_loss)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XF04k3XT-x-A",
        "outputId": "387718c8-adcd-4dae-fff3-c6efa134900e"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(30, 5)\n",
            "Epoch 1/100\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.6323 - val_loss: 0.5259\n",
            "Epoch 2/100\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.6003 - val_loss: 0.5075\n",
            "Epoch 3/100\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.5712 - val_loss: 0.4908\n",
            "Epoch 4/100\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.5443 - val_loss: 0.4753\n",
            "Epoch 5/100\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.5189 - val_loss: 0.4605\n",
            "Epoch 6/100\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.4948 - val_loss: 0.4462\n",
            "Epoch 7/100\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.4716 - val_loss: 0.4323\n",
            "Epoch 8/100\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.4492 - val_loss: 0.4187\n",
            "Epoch 9/100\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.4275 - val_loss: 0.4053\n",
            "Epoch 10/100\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.4065 - val_loss: 0.3920\n",
            "Epoch 11/100\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.3859 - val_loss: 0.3788\n",
            "Epoch 12/100\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.3658 - val_loss: 0.3656\n",
            "Epoch 13/100\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.3464 - val_loss: 0.3523\n",
            "Epoch 14/100\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.3274 - val_loss: 0.3387\n",
            "Epoch 15/100\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.3089 - val_loss: 0.3248\n",
            "Epoch 16/100\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.2907 - val_loss: 0.3103\n",
            "Epoch 17/100\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.2725 - val_loss: 0.2950\n",
            "Epoch 18/100\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.2541 - val_loss: 0.2784\n",
            "Epoch 19/100\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.2353 - val_loss: 0.2603\n",
            "Epoch 20/100\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.2157 - val_loss: 0.2403\n",
            "Epoch 21/100\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.1952 - val_loss: 0.2191\n",
            "Epoch 22/100\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.1737 - val_loss: 0.1987\n",
            "Epoch 23/100\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.1527 - val_loss: 0.1820\n",
            "Epoch 24/100\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.1346 - val_loss: 0.1699\n",
            "Epoch 25/100\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.1211 - val_loss: 0.1613\n",
            "Epoch 26/100\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.1113 - val_loss: 0.1545\n",
            "Epoch 27/100\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.1038 - val_loss: 0.1486\n",
            "Epoch 28/100\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.0976 - val_loss: 0.1432\n",
            "Epoch 29/100\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.0922 - val_loss: 0.1380\n",
            "Epoch 30/100\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.0873 - val_loss: 0.1330\n",
            "Epoch 31/100\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.0829 - val_loss: 0.1281\n",
            "Epoch 32/100\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.0789 - val_loss: 0.1234\n",
            "Epoch 33/100\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.0753 - val_loss: 0.1189\n",
            "Epoch 34/100\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.0721 - val_loss: 0.1145\n",
            "Epoch 35/100\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.0692 - val_loss: 0.1104\n",
            "Epoch 36/100\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.0667 - val_loss: 0.1066\n",
            "Epoch 37/100\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.0645 - val_loss: 0.1030\n",
            "Epoch 38/100\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.0628 - val_loss: 0.0997\n",
            "Epoch 39/100\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.0613 - val_loss: 0.0966\n",
            "Epoch 40/100\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0601 - val_loss: 0.0936\n",
            "Epoch 41/100\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.0591 - val_loss: 0.0908\n",
            "Epoch 42/100\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.0583 - val_loss: 0.0881\n",
            "Epoch 43/100\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.0576 - val_loss: 0.0855\n",
            "Epoch 44/100\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.0569 - val_loss: 0.0830\n",
            "Epoch 45/100\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.0563 - val_loss: 0.0806\n",
            "Epoch 46/100\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.0556 - val_loss: 0.0783\n",
            "Epoch 47/100\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.0550 - val_loss: 0.0762\n",
            "Epoch 48/100\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.0544 - val_loss: 0.0744\n",
            "Epoch 49/100\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.0538 - val_loss: 0.0728\n",
            "Epoch 50/100\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.0531 - val_loss: 0.0713\n",
            "Epoch 51/100\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.0524 - val_loss: 0.0698\n",
            "Epoch 52/100\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.0518 - val_loss: 0.0684\n",
            "Epoch 53/100\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.0511 - val_loss: 0.0670\n",
            "Epoch 54/100\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.0504 - val_loss: 0.0658\n",
            "Epoch 55/100\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.0498 - val_loss: 0.0646\n",
            "Epoch 56/100\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.0492 - val_loss: 0.0634\n",
            "Epoch 57/100\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.0485 - val_loss: 0.0624\n",
            "Epoch 58/100\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.0479 - val_loss: 0.0614\n",
            "Epoch 59/100\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.0474 - val_loss: 0.0605\n",
            "Epoch 60/100\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.0468 - val_loss: 0.0596\n",
            "Epoch 61/100\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.0463 - val_loss: 0.0588\n",
            "Epoch 62/100\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.0458 - val_loss: 0.0581\n",
            "Epoch 63/100\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.0454 - val_loss: 0.0574\n",
            "Epoch 64/100\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.0450 - val_loss: 0.0566\n",
            "Epoch 65/100\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.0446 - val_loss: 0.0559\n",
            "Epoch 66/100\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.0442 - val_loss: 0.0552\n",
            "Epoch 67/100\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.0438 - val_loss: 0.0545\n",
            "Epoch 68/100\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.0433 - val_loss: 0.0538\n",
            "Epoch 69/100\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.0429 - val_loss: 0.0531\n",
            "Epoch 70/100\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.0426 - val_loss: 0.0524\n",
            "Epoch 71/100\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.0422 - val_loss: 0.0517\n",
            "Epoch 72/100\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.0418 - val_loss: 0.0511\n",
            "Epoch 73/100\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.0415 - val_loss: 0.0505\n",
            "Epoch 74/100\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.0411 - val_loss: 0.0499\n",
            "Epoch 75/100\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.0407 - val_loss: 0.0494\n",
            "Epoch 76/100\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.0403 - val_loss: 0.0488\n",
            "Epoch 77/100\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.0400 - val_loss: 0.0483\n",
            "Epoch 78/100\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.0396 - val_loss: 0.0478\n",
            "Epoch 79/100\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.0392 - val_loss: 0.0473\n",
            "Epoch 80/100\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.0389 - val_loss: 0.0468\n",
            "Epoch 81/100\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.0385 - val_loss: 0.0463\n",
            "Epoch 82/100\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.0382 - val_loss: 0.0459\n",
            "Epoch 83/100\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.0378 - val_loss: 0.0454\n",
            "Epoch 84/100\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.0375 - val_loss: 0.0449\n",
            "Epoch 85/100\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.0371 - val_loss: 0.0444\n",
            "Epoch 86/100\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.0367 - val_loss: 0.0438\n",
            "Epoch 87/100\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.0364 - val_loss: 0.0433\n",
            "Epoch 88/100\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.0360 - val_loss: 0.0428\n",
            "Epoch 89/100\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.0357 - val_loss: 0.0423\n",
            "Epoch 90/100\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.0353 - val_loss: 0.0418\n",
            "Epoch 91/100\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.0350 - val_loss: 0.0413\n",
            "Epoch 92/100\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.0346 - val_loss: 0.0408\n",
            "Epoch 93/100\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.0343 - val_loss: 0.0403\n",
            "Epoch 94/100\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.0339 - val_loss: 0.0398\n",
            "Epoch 95/100\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.0336 - val_loss: 0.0394\n",
            "Epoch 96/100\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.0332 - val_loss: 0.0389\n",
            "Epoch 97/100\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.0328 - val_loss: 0.0385\n",
            "Epoch 98/100\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.0325 - val_loss: 0.0381\n",
            "Epoch 99/100\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.0321 - val_loss: 0.0376\n",
            "Epoch 100/100\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.0318 - val_loss: 0.0372\n",
            "1/1 [==============================] - 0s 445ms/step\n",
            "[[178.90646  183.27031  131.64291  115.73963 ]\n",
            " [293.2288   308.23386  266.52084  274.19028 ]\n",
            " [250.24048  249.77518  276.9776   240.65164 ]\n",
            " [239.46138  238.28816  218.04     198.85825 ]\n",
            " [198.17317  208.23077  113.86542  119.421776]\n",
            " [173.5911   180.32626   95.60222   94.01476 ]]\n",
            "Best Validation Loss: 0.03718530014157295\n",
            "Best Training Loss: 0.0317767895758152\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "today_date = datetime.strptime('2024-03-11', '%Y-%m-%d')\n",
        "# Calculate DTE for the next day\n",
        "next_day_dte = 2\n",
        "\n",
        "latest_index_data = historical_data[historical_data['Date'] == '11/03/2024']\n",
        "#print(latest_index_data)\n",
        "latest_index_high = latest_index_data['index_High'].values\n",
        "latest_index_low = latest_index_data['index_Low'].values\n",
        "latest_index_close = latest_index_data['index_Close'].values\n",
        "latest_index_open = latest_index_data['index_Open'].values\n",
        "latest_index_gap = latest_index_data['index_Gap'].values\n",
        "\n",
        "\n",
        "latest_index_open = np.array(latest_index_open).reshape(-1, 1)\n",
        "#print(latest_index_open.shape)  # has hape (0,1)\n",
        "latest_index_high = np.array(latest_index_high).reshape(-1, 1)\n",
        "latest_index_low = np.array(latest_index_low).reshape(-1, 1)\n",
        "latest_index_close = np.array(latest_index_close).reshape(-1, 1)\n",
        "latest_index_gap = np.array(latest_index_gap).reshape(-1, 1)\n",
        "\n",
        "# #next day features should have shape of 1,5\n",
        "# next_day_features = np.array([latest_index_open, latest_index_high, latest_index_low, latest_index_close , latest_index_gap])\n",
        "# print(next_day_features.shape)\n",
        "next_day_features = np.hstack((latest_index_open, latest_index_high, latest_index_low, latest_index_close, latest_index_gap))\n",
        "print(next_day_features.shape)  # Should print (1, 5)\n",
        "\n",
        "next_day_features_scaled = scaler_X.fit_transform(next_day_features)\n",
        "\n",
        "array_1x1 = np.array([[2]])\n",
        "next_day_features_scaled = np.hstack((next_day_features_scaled, array_1x1))\n",
        "# Reshape input data for LSTM\n",
        "next_day_features_scaled = next_day_features_scaled.reshape((next_day_features_scaled.shape[0], 1, next_day_features_scaled.shape[1]))\n",
        "# Predict Straddle Open, High, Low, Close for the next day\n",
        "next_day_predictions_scaled = model.predict(next_day_features_scaled)\n",
        "next_day_predictions_unscaled = scaler_y.inverse_transform(next_day_predictions_scaled)\n",
        "print('Predicted Straddle Open, High, Low, Close for the next day for 2dte:', next_day_predictions_unscaled)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmwGghnRP5gw",
        "outputId": "7cdb34c1-6123-4c42-937c-aa463a7b2126"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 5)\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Predicted Straddle Open, High, Low, Close for the next day: [[170.34102  171.92511  122.06615  102.704414]]\n"
          ]
        }
      ]
    }
  ]
}